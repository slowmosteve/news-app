{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Topic Modeling\n",
    "\n",
    "This notebook is used to derive topics from news text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Activated [news-site].\n"
    }
   ],
   "source": [
    "!gcloud config configurations activate news-site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import spacy\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import en_core_web_md\n",
    "\n",
    "import pandas\n",
    "import re\n",
    "import numpy\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;\nRangeIndex: 2378 entries, 0 to 2377\nData columns (total 3 columns):\n #   Column       Non-Null Count  Dtype              \n---  ------       --------------  -----              \n 0   article_id   2378 non-null   object             \n 1   publishedAt  2378 non-null   datetime64[ns, UTC]\n 2   text         2378 non-null   object             \ndtypes: datetime64[ns, UTC](1), object(2)\nmemory usage: 55.9+ KB\n"
    }
   ],
   "source": [
    "client = bigquery.Client()\n",
    "sql = \"\"\"\n",
    "    SELECT \n",
    "        article_id,\n",
    "        publishedAt,\n",
    "        CONCAT(title, '. ', description, '. ', content) AS text\n",
    "    FROM `news-site-280319.news.articles`\n",
    "    WHERE\n",
    "      title IS NOT NULL\n",
    "      AND description IS NOT NULL\n",
    "      AND content IS NOT NULL\n",
    "      AND DATE(publishedAt) >= DATE_SUB(CURRENT_DATE(), INTERVAL 14 DAY)\n",
    "\"\"\"\n",
    "\n",
    "df = client.query(sql).to_dataframe()\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0    2020/09/26 10:00 GMT. The latest five minute n...\n1    2020/09/26 11:00 GMT. The latest five minute n...\n2    Fleetwood Town v AFC Wimbledon. Live coverage ...\n3    Queens Park Rangers v Middlesbrough. Live cove...\n4    Millwall v Brentford. Live coverage of Saturda...\nName: text, dtype: object"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "news_text = df['text']\n",
    "news_text.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace all new line returns with spaces\n",
    "news_text = news_text.str.replace('\\r\\n', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to lowercase\n",
    "news_text = news_text.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(doc):\n",
    "    doc = [token.lemma_ for token in doc if token.lemma_ != '-PRON-']\n",
    "    doc = u' '.join(doc)\n",
    "    return nlp.make_doc(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update stop words list\n",
    "custom_stop_list = [\"char\", \"   \", \"  \", \"|\", \"reuters\"]\n",
    "nlp.Defaults.stop_words.update(custom_stop_list)\n",
    "\n",
    "for word in STOP_WORDS:\n",
    "    lexeme = nlp.vocab[word]\n",
    "    lexeme.is_stop = True\n",
    "\n",
    "def remove_stopwords(doc):\n",
    "    # This will remove stopwords and punctuation.\n",
    "    # Use token.text to return strings, which we'll need for Gensim.\n",
    "    doc = [token.text for token in doc if token.is_stop != True and token.is_punct != True]\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The add_pipe function appends our functions to the default pipeline.\n",
    "nlp.add_pipe(lemmatizer,name='lemmatizer',after='ner')\n",
    "nlp.add_pipe(remove_stopwords, name=\"stopwords\", last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = []\n",
    "# Iterates through each article in the corpus.\n",
    "for doc in news_text:\n",
    "    # Passes that article through the pipeline and adds to a new list.\n",
    "    pr = nlp(doc)\n",
    "    doc_list.append(pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates mapping of word IDs to words.\n",
    "words = corpora.Dictionary(doc_list)\n",
    "\n",
    "# Turns each document into a bag of words.\n",
    "corpus = [words.doc2bow(doc) for doc in doc_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=words,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=0,\n",
    "                                           update_every=1,\n",
    "                                           passes=100,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[(0,\n  &#39;0.014*&quot;&gt;&quot; + 0.010*&quot;&lt;&quot; + 0.010*&quot;talk&quot; + 0.010*&quot;capacity&quot; + 0.009*&quot;antibody&quot; &#39;\n  &#39;+ 0.008*&quot;september&quot; + 0.007*&quot;month&quot; + 0.007*&quot;chelsea&quot; + 0.007*&quot;continue&quot; + &#39;\n  &#39;0.007*&quot;defender&quot;&#39;),\n (1,\n  &#39;0.016*&quot;court&quot; + 0.013*&quot;supreme&quot; + 0.012*&quot;sound&quot; + 0.012*&quot;death&quot; + &#39;\n  &#39;0.011*&quot;party&quot; + 0.010*&quot;ginsburg&quot; + 0.009*&quot;justice&quot; + 0.008*&quot;ruth&quot; + &#39;\n  &#39;0.008*&quot;bader&quot; + 0.008*&quot;disaster&quot;&#39;),\n (2,\n  &#39;0.022*&quot;united&quot; + 0.011*&quot;gas&quot; + 0.010*&quot;states&quot; + 0.010*&quot;friday&quot; + &#39;\n  &#39;0.010*&quot;report&quot; + 0.009*&quot;cut&quot; + 0.008*&quot;stake&quot; + 0.008*&quot;france&quot; + &#39;\n  &#39;0.008*&quot;police&quot; + 0.008*&quot;people&quot;&#39;),\n (3,\n  &#39;0.011*&quot;win&quot; + 0.010*&quot;deal&quot; + 0.010*&quot;big&quot; + 0.010*&quot;new&quot; + 0.009*&quot;5&quot; + &#39;\n  &#39;0.008*&quot;score&quot; + 0.008*&quot;everton&quot; + 0.008*&quot;kentucky&quot; + 0.008*&quot;pass&quot; + &#39;\n  &#39;0.007*&quot;saturday&quot;&#39;),\n (4,\n  &#39;0.019*&quot;u.s&quot; + 0.019*&quot;year&quot; + 0.016*&quot;future&quot; + 0.015*&quot;company&quot; + 0.015*&quot;oil&quot; &#39;\n  &#39;+ 0.013*&quot;million&quot; + 0.012*&quot;china&quot; + 0.012*&quot;rise&quot; + 0.010*&quot;market&quot; + &#39;\n  &#39;0.009*&quot;$&quot;&#39;),\n (5,\n  &#39;0.027*&quot;bank&quot; + 0.011*&quot;central&quot; + 0.011*&quot;near&quot; + 0.010*&quot;paris&quot; + &#39;\n  &#39;0.010*&quot;september&quot; + 0.010*&quot;fire&quot; + 0.009*&quot;staff&quot; + 0.008*&quot;french&quot; + &#39;\n  &#39;0.008*&quot;source&quot; + 0.008*&quot;tell&quot;&#39;),\n (6,\n  &#39;0.011*&quot;year&quot; + 0.009*&quot;deal&quot; + 0.009*&quot;south&quot; + 0.008*&quot;pandemic&quot; + &#39;\n  &#39;0.007*&quot;new&quot; + 0.007*&quot;sign&quot; + 0.007*&quot;trade&quot; + 0.007*&quot;base&quot; + 0.007*&quot;product&quot; &#39;\n  &#39;+ 0.006*&quot;crisis&quot;&#39;),\n (7,\n  &#39;0.025*&quot;senate&quot; + 0.025*&quot;court&quot; + 0.021*&quot;trump&quot; + 0.021*&quot;supreme&quot; + &#39;\n  &#39;0.012*&quot;mcconnell&quot; + 0.012*&quot;u.s&quot; + 0.012*&quot;president&quot; + 0.012*&quot;barrett&quot; + &#39;\n  &#39;0.012*&quot;majority&quot; + 0.012*&quot;amy&quot;&#39;),\n (8,\n  &#39;0.027*&quot;trump&quot; + 0.024*&quot;biden&quot; + 0.021*&quot;election&quot; + 0.014*&quot;president&quot; + &#39;\n  &#39;0.013*&quot;joe&quot; + 0.012*&quot;state&quot; + 0.011*&quot;debate&quot; + 0.010*&quot;face&quot; + &#39;\n  &#39;0.010*&quot;donald&quot; + 0.010*&quot;u.s&quot;&#39;),\n (9,\n  &#39;0.014*&quot;bank&quot; + 0.013*&quot;england&quot; + 0.012*&quot;united&quot; + 0.012*&quot;london&quot; + &#39;\n  &#39;0.010*&quot;uk&quot; + 0.010*&quot;britain&quot; + 0.010*&quot;manchester&quot; + 0.009*&quot;financial&quot; + &#39;\n  &#39;0.008*&quot;hamilton&quot; + 0.008*&quot;club&quot;&#39;),\n (10,\n  &#39;0.040*&quot;coronavirus&quot; + 0.034*&quot;covid-19&quot; + 0.024*&quot;case&quot; + 0.016*&quot;new&quot; + &#39;\n  &#39;0.015*&quot;pandemic&quot; + 0.012*&quot;record&quot; + 0.011*&quot;high&quot; + 0.011*&quot;report&quot; + &#39;\n  &#39;0.011*&quot;vaccine&quot; + 0.008*&quot;staff&quot;&#39;),\n (11,\n  &#39;0.027*&quot;world&quot; + 0.021*&quot;police&quot; + 0.021*&quot;late&quot; + 0.021*&quot;service&quot; + &#39;\n  &#39;0.021*&quot;bbc&quot; + 0.020*&quot;protest&quot; + 0.019*&quot;news&quot; + 0.018*&quot;minute&quot; + &#39;\n  &#39;0.015*&quot;bulletin&quot; + 0.008*&quot;gmt&quot;&#39;),\n (12,\n  &#39;0.019*&quot;open&quot; + 0.014*&quot;french&quot; + 0.010*&quot;italian&quot; + 0.010*&quot;ap&quot; + &#39;\n  &#39;0.010*&quot;round&quot; + 0.010*&quot;time&quot; + 0.009*&quot;indian&quot; + 0.009*&quot;match&quot; + &#39;\n  &#39;0.008*&quot;sunday&quot; + 0.007*&quot;saturday&quot;&#39;),\n (13,\n  &#39;0.021*&quot;game&quot; + 0.017*&quot;season&quot; + 0.015*&quot;new&quot; + 0.013*&quot;saturday&quot; + &#39;\n  &#39;0.011*&quot;touchdown&quot; + 0.010*&quot;nfl&quot; + 0.010*&quot;video&quot; + 0.010*&quot;week&quot; + &#39;\n  &#39;0.010*&quot;team&quot; + 0.009*&quot;ap&quot;&#39;),\n (14,\n  &#39;0.017*&quot;law&quot; + 0.016*&quot;player&quot; + 0.014*&quot;de&quot; + 0.012*&quot;tiktok&quot; + 0.011*&quot;union&quot; &#39;\n  &#39;+ 0.011*&quot;storm&quot; + 0.011*&quot;break&quot; + 0.011*&quot;ban&quot; + 0.010*&quot;government&quot; + &#39;\n  &#39;0.009*&quot;app&quot;&#39;),\n (15,\n  &#39;0.017*&quot;week&quot; + 0.011*&quot;u.s&quot; + 0.010*&quot;world&quot; + 0.009*&quot;russia&quot; + &#39;\n  &#39;0.008*&quot;tuesday&quot; + 0.008*&quot;amazon&quot; + 0.008*&quot;opposition&quot; + 0.007*&quot;leader&quot; + &#39;\n  &#39;0.006*&quot;stock&quot; + 0.006*&quot;time&quot;&#39;),\n (16,\n  &#39;0.050*&quot;trump&quot; + 0.041*&quot;president&quot; + 0.015*&quot;house&quot; + 0.014*&quot;covid-19&quot; + &#39;\n  &#39;0.013*&quot;donald&quot; + 0.011*&quot;white&quot; + 0.010*&quot;test&quot; + 0.010*&quot;coronavirus&quot; + &#39;\n  &#39;0.008*&quot;u.s&quot; + 0.008*&quot;positive&quot;&#39;),\n (17,\n  &#39;0.027*&quot;$&quot; + 0.020*&quot;billion&quot; + 0.013*&quot;south&quot; + 0.010*&quot;group&quot; + 0.008*&quot;court&quot; &#39;\n  &#39;+ 0.008*&quot;person&quot; + 0.008*&quot;monday&quot; + 0.007*&quot;500&quot; + 0.007*&quot;ask&quot; + &#39;\n  &#39;0.007*&quot;set&quot;&#39;),\n (18,\n  &#39;0.022*&quot;minister&quot; + 0.018*&quot;staff&quot; + 0.015*&quot;photo&quot; + 0.015*&quot;file&quot; + &#39;\n  &#39;0.014*&quot;prime&quot; + 0.013*&quot;new&quot; + 0.012*&quot;uk&quot; + 0.009*&quot;johnson&quot; + 0.009*&quot;apple&quot; &#39;\n  &#39;+ 0.008*&quot;2020&quot;&#39;),\n (19,\n  &#39;0.027*&quot;live&quot; + 0.021*&quot;ufc&quot; + 0.018*&quot;result&quot; + 0.017*&quot;airline&quot; + 0.014*&quot;las&quot; &#39;\n  &#39;+ 0.013*&quot;vegas&quot; + 0.012*&quot;9&quot; + 0.011*&quot;weigh&quot; + 0.011*&quot;airways&quot; + &#39;\n  &#39;0.010*&quot;a.m.&quot;&#39;)]\n"
    }
   ],
   "source": [
    "pprint(lda_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   document_no  dominant_topic  topic_perc_contrib  \\\n0            0            11.0              0.9344   \n1            1            11.0              0.9344   \n2            2            13.0              0.8615   \n3            3            13.0              0.7713   \n4            4            13.0              0.5380   \n\n                                            keywords  \\\n0  world, police, late, service, bbc, protest, ne...   \n1  world, police, late, service, bbc, protest, ne...   \n2  game, season, new, saturday, touchdown, nfl, v...   \n3  game, season, new, saturday, touchdown, nfl, v...   \n4  game, season, new, saturday, touchdown, nfl, v...   \n\n                                                text  \n0  2020/09/26 10:00 gmt. the latest five minute n...  \n1  2020/09/26 11:00 gmt. the latest five minute n...  \n2  fleetwood town v afc wimbledon. live coverage ...  \n3  queens park rangers v middlesbrough. live cove...  \n4  millwall v brentford. live coverage of saturda...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>document_no</th>\n      <th>dominant_topic</th>\n      <th>topic_perc_contrib</th>\n      <th>keywords</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>11.0</td>\n      <td>0.9344</td>\n      <td>world, police, late, service, bbc, protest, ne...</td>\n      <td>2020/09/26 10:00 gmt. the latest five minute n...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>11.0</td>\n      <td>0.9344</td>\n      <td>world, police, late, service, bbc, protest, ne...</td>\n      <td>2020/09/26 11:00 gmt. the latest five minute n...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>13.0</td>\n      <td>0.8615</td>\n      <td>game, season, new, saturday, touchdown, nfl, v...</td>\n      <td>fleetwood town v afc wimbledon. live coverage ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>13.0</td>\n      <td>0.7713</td>\n      <td>game, season, new, saturday, touchdown, nfl, v...</td>\n      <td>queens park rangers v middlesbrough. live cove...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>13.0</td>\n      <td>0.5380</td>\n      <td>game, season, new, saturday, touchdown, nfl, v...</td>\n      <td>millwall v brentford. live coverage of saturda...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "def format_topics_sentences(ldamodel, corpus, texts):\n",
    "    # Init output\n",
    "    sent_topics_df = pandas.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row[0], key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(\n",
    "                        pandas.Series(\n",
    "                            [int(topic_num), round(prop_topic,4),topic_keywords]\n",
    "                        ), ignore_index=True\n",
    "                    )\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['dominant_topic', 'perc_contribution', 'topic_keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pandas.Series(texts)\n",
    "    sent_topics_df = pandas.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=news_text)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['document_no', 'dominant_topic', 'topic_perc_contrib', 'keywords', 'text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Group top 5 sentences under each topic\n",
    "# sent_topics_sorteddf = pandas.DataFrame()\n",
    "\n",
    "# sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('dominant_topic')\n",
    "\n",
    "# for i, grp in sent_topics_outdf_grpd:\n",
    "#     sent_topics_sorteddf = pandas.concat([sent_topics_sorteddf, \n",
    "#                                              grp.sort_values(['perc_contribution'], \n",
    "#                                              ascending=[0]).head(1)],\n",
    "#                                              axis=0\n",
    "#                                              )\n",
    "\n",
    "# # Reset Index    \n",
    "# sent_topics_sorteddf.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# # Format\n",
    "# sent_topics_sorteddf.columns = ['topic_Num', 'perc_Contribution', 'keywords', 'text']\n",
    "\n",
    "# # Show\n",
    "# sent_topics_sorteddf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   dominant_topic                                     topic_keywords  \\\n4             4.0  u.s, year, future, company, oil, million, chin...   \n3             3.0  win, deal, big, new, 5, score, everton, kentuc...   \n1             1.0  court, supreme, sound, death, party, ginsburg,...   \n2             2.0  united, gas, states, friday, report, cut, stak...   \n0             0.0  &gt;, &lt;, talk, capacity, antibody, september, mon...   \n\n   num_documents  perc_documents  \n4            132        5.550883  \n3            111        4.667788  \n1             94        3.952902  \n2             84        3.532380  \n0             75        3.153911  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dominant_topic</th>\n      <th>topic_keywords</th>\n      <th>num_documents</th>\n      <th>perc_documents</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4</th>\n      <td>4.0</td>\n      <td>u.s, year, future, company, oil, million, chin...</td>\n      <td>132</td>\n      <td>5.550883</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3.0</td>\n      <td>win, deal, big, new, 5, score, everton, kentuc...</td>\n      <td>111</td>\n      <td>4.667788</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n      <td>court, supreme, sound, death, party, ginsburg,...</td>\n      <td>94</td>\n      <td>3.952902</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.0</td>\n      <td>united, gas, states, friday, report, cut, stak...</td>\n      <td>84</td>\n      <td>3.532380</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>&gt;, &lt;, talk, capacity, antibody, september, mon...</td>\n      <td>75</td>\n      <td>3.153911</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "# Number of Documents for Each Topic\n",
    "topic_counts = df_topic_sents_keywords['dominant_topic'].value_counts()\n",
    "\n",
    "# Topic Number and Keywords\n",
    "topic_num_keywords = df_topic_sents_keywords[['dominant_topic', 'topic_keywords']].groupby(\n",
    "                        ['dominant_topic', 'topic_keywords']\n",
    "                    )['topic_keywords'].count().reset_index(name='num_documents')\n",
    "\n",
    "topic_num_keywords['perc_documents'] = ((topic_num_keywords['num_documents'])/(topic_num_keywords['num_documents'].sum()))*100\n",
    "\n",
    "# show top topics\n",
    "topic_num_keywords.head(5).sort_values(by='perc_documents', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                article_id               publishedAt  \\\n0     7e68d7c5-b7e8-46bc-a25e-ccd5b39c921e 2020-09-26 10:06:00+00:00   \n1     8313acac-487a-4437-86f0-2882fedaf907 2020-09-26 11:06:00+00:00   \n2     c65d1095-ebc1-41a4-871e-e8575c2c8859 2020-09-26 10:44:53+00:00   \n3     0923ad9f-e2de-43d3-a864-89fcb4dd60f8 2020-09-26 10:43:27+00:00   \n4     246ddb7a-27ec-40a4-afdc-bd45bdd6ea3c 2020-09-26 10:43:25+00:00   \n...                                    ...                       ...   \n2373  7a27e564-540e-4464-a5df-421d3d3da6c4 2020-10-03 21:02:33+00:00   \n2374  9cca6cab-f844-484c-9372-4c9274a7e22e 2020-10-03 20:36:42+00:00   \n2375  af4b3347-d937-40bc-9d10-02c73ed15dc2 2020-10-03 20:08:42+00:00   \n2376  82b7aeda-d162-44a2-9444-2f27df8b181b 2020-10-03 20:43:01+00:00   \n2377  92618445-922a-4c95-9903-5e0e2a68ec54 2020-09-27 11:49:37+00:00   \n\n                                                   text  dominant_topic  \\\n0     2020/09/26 10:00 gmt. the latest five minute n...              11   \n1     2020/09/26 11:00 gmt. the latest five minute n...              11   \n2     fleetwood town v afc wimbledon. live coverage ...              13   \n3     queens park rangers v middlesbrough. live cove...              13   \n4     millwall v brentford. live coverage of saturda...              13   \n...                                                 ...             ...   \n2373  no. 3 florida keeps rolling with defeat of sou...               8   \n2374  jennifer aniston loves these sweaty betty legg...              19   \n2375  colliding crises shake already chaotic campaig...               1   \n2376  was trump ever on oxygen? health, security exp...              16   \n2377  grocers stockpile, build &#39;pandemic pallets&#39;......              16   \n\n      topic_perc_contrib                                           keywords  \n0                 0.9344  world, police, late, service, bbc, protest, ne...  \n1                 0.9344  world, police, late, service, bbc, protest, ne...  \n2                 0.8615  game, season, new, saturday, touchdown, nfl, v...  \n3                 0.7713  game, season, new, saturday, touchdown, nfl, v...  \n4                 0.5380  game, season, new, saturday, touchdown, nfl, v...  \n...                  ...                                                ...  \n2373              0.7854  trump, biden, election, president, joe, state,...  \n2374              0.4395  live, ufc, result, airline, las, vegas, 9, wei...  \n2375              0.3522  court, supreme, sound, death, party, ginsburg,...  \n2376              0.5241  trump, president, house, covid-19, donald, whi...  \n2377              0.3333  trump, president, house, covid-19, donald, whi...  \n\n[2378 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>article_id</th>\n      <th>publishedAt</th>\n      <th>text</th>\n      <th>dominant_topic</th>\n      <th>topic_perc_contrib</th>\n      <th>keywords</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7e68d7c5-b7e8-46bc-a25e-ccd5b39c921e</td>\n      <td>2020-09-26 10:06:00+00:00</td>\n      <td>2020/09/26 10:00 gmt. the latest five minute n...</td>\n      <td>11</td>\n      <td>0.9344</td>\n      <td>world, police, late, service, bbc, protest, ne...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8313acac-487a-4437-86f0-2882fedaf907</td>\n      <td>2020-09-26 11:06:00+00:00</td>\n      <td>2020/09/26 11:00 gmt. the latest five minute n...</td>\n      <td>11</td>\n      <td>0.9344</td>\n      <td>world, police, late, service, bbc, protest, ne...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>c65d1095-ebc1-41a4-871e-e8575c2c8859</td>\n      <td>2020-09-26 10:44:53+00:00</td>\n      <td>fleetwood town v afc wimbledon. live coverage ...</td>\n      <td>13</td>\n      <td>0.8615</td>\n      <td>game, season, new, saturday, touchdown, nfl, v...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0923ad9f-e2de-43d3-a864-89fcb4dd60f8</td>\n      <td>2020-09-26 10:43:27+00:00</td>\n      <td>queens park rangers v middlesbrough. live cove...</td>\n      <td>13</td>\n      <td>0.7713</td>\n      <td>game, season, new, saturday, touchdown, nfl, v...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>246ddb7a-27ec-40a4-afdc-bd45bdd6ea3c</td>\n      <td>2020-09-26 10:43:25+00:00</td>\n      <td>millwall v brentford. live coverage of saturda...</td>\n      <td>13</td>\n      <td>0.5380</td>\n      <td>game, season, new, saturday, touchdown, nfl, v...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2373</th>\n      <td>7a27e564-540e-4464-a5df-421d3d3da6c4</td>\n      <td>2020-10-03 21:02:33+00:00</td>\n      <td>no. 3 florida keeps rolling with defeat of sou...</td>\n      <td>8</td>\n      <td>0.7854</td>\n      <td>trump, biden, election, president, joe, state,...</td>\n    </tr>\n    <tr>\n      <th>2374</th>\n      <td>9cca6cab-f844-484c-9372-4c9274a7e22e</td>\n      <td>2020-10-03 20:36:42+00:00</td>\n      <td>jennifer aniston loves these sweaty betty legg...</td>\n      <td>19</td>\n      <td>0.4395</td>\n      <td>live, ufc, result, airline, las, vegas, 9, wei...</td>\n    </tr>\n    <tr>\n      <th>2375</th>\n      <td>af4b3347-d937-40bc-9d10-02c73ed15dc2</td>\n      <td>2020-10-03 20:08:42+00:00</td>\n      <td>colliding crises shake already chaotic campaig...</td>\n      <td>1</td>\n      <td>0.3522</td>\n      <td>court, supreme, sound, death, party, ginsburg,...</td>\n    </tr>\n    <tr>\n      <th>2376</th>\n      <td>82b7aeda-d162-44a2-9444-2f27df8b181b</td>\n      <td>2020-10-03 20:43:01+00:00</td>\n      <td>was trump ever on oxygen? health, security exp...</td>\n      <td>16</td>\n      <td>0.5241</td>\n      <td>trump, president, house, covid-19, donald, whi...</td>\n    </tr>\n    <tr>\n      <th>2377</th>\n      <td>92618445-922a-4c95-9903-5e0e2a68ec54</td>\n      <td>2020-09-27 11:49:37+00:00</td>\n      <td>grocers stockpile, build 'pandemic pallets'......</td>\n      <td>16</td>\n      <td>0.3333</td>\n      <td>trump, president, house, covid-19, donald, whi...</td>\n    </tr>\n  </tbody>\n</table>\n<p>2378 rows Ã— 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "output_df = pandas.concat([df[['article_id', 'publishedAt']], news_text, df_dominant_topic], axis=1)\n",
    "\n",
    "output_df.columns = ['article_id', 'publishedAt', 'text', 'drop', 'dominant_topic', 'topic_perc_contrib', 'keywords', 'drop']\n",
    "\n",
    "output_df.drop(output_df['drop'], axis=1, inplace=True)\n",
    "\n",
    "output_df['dominant_topic'] = output_df['dominant_topic'].astype(int)\n",
    "\n",
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure BigQuery job\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    # Specify a (partial) schema. All columns are always written to the\n",
    "    # table. The schema is used to assist in data type definitions.\n",
    "    schema=[\n",
    "        # Specify the type of columns whose type cannot be auto-detected. For\n",
    "        # example the \"title\" column uses pandas dtype \"object\", so its\n",
    "        # data type is ambiguous.\n",
    "        bigquery.SchemaField(\"article_id\", bigquery.enums.SqlTypeNames.STRING),\n",
    "        bigquery.SchemaField(\"publishedAt\", bigquery.enums.SqlTypeNames.TIMESTAMP),\n",
    "        bigquery.SchemaField(\"text\", bigquery.enums.SqlTypeNames.STRING),\n",
    "        bigquery.SchemaField(\"dominant_topic\", bigquery.enums.SqlTypeNames.INT64),\n",
    "        bigquery.SchemaField(\"topic_perc_contrib\", bigquery.enums.SqlTypeNames.FLOAT64),\n",
    "        bigquery.SchemaField(\"keywords\", bigquery.enums.SqlTypeNames.STRING),\n",
    "    ],\n",
    "    # Optionally, set the write disposition. BigQuery appends loaded rows\n",
    "    # to an existing table by default, but with WRITE_TRUNCATE write\n",
    "    # disposition it replaces the table with the loaded data.\n",
    "    write_disposition=\"WRITE_TRUNCATE\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "&lt;google.cloud.bigquery.job.LoadJob at 0x1328abd60&gt;"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "table_id = \"news-site-280319.topics.article_topics\"\n",
    "job = client.load_table_from_dataframe(\n",
    "    output_df,\n",
    "    table_id, \n",
    "    job_config=job_config\n",
    ")  # Make an API request.\n",
    "job.result()  # Wait for the job to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "&#39;DONE&#39;"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "# check job status\n",
    "job.result().state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.0 64-bit",
   "display_name": "Python 3.8.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "1e9b8f06d66bfd1d9120cfb816eef09b5497da80937f660f45e67f272e6075a8"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}